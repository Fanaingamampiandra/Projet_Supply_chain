{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e958859e",
   "metadata": {},
   "source": [
    "# Preprossessing & Machine Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7091ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb # pip install xgboost\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from statsmodels.tsa.arima.model import ARIMA # Pour ARIMA\n",
    "from prophet import Prophet # pip install prophet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pour la reproductibilité\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d29aae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data \n",
    "df_global=pd.read_csv(\"../code/achat_prod_fournisseur_stock.csv\") #upload from code file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c56526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 22 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   id_achat               10000 non-null  object        \n",
      " 1   date_achat             10000 non-null  datetime64[ns]\n",
      " 2   id_produit             10000 non-null  object        \n",
      " 3   quantité               10000 non-null  int64         \n",
      " 4   id_fournisseur         10000 non-null  object        \n",
      " 5   prix_unitaire          10000 non-null  float64       \n",
      " 6   délai_livraison_jours  10000 non-null  int64         \n",
      " 7   montant_total          10000 non-null  float64       \n",
      " 8   mois                   10000 non-null  int64         \n",
      " 9   année                  10000 non-null  int64         \n",
      " 10  jour_semaine           10000 non-null  int64         \n",
      " 11  catégorie              10000 non-null  object        \n",
      " 12  marque                 10000 non-null  object        \n",
      " 13  prix                   10000 non-null  float64       \n",
      " 14  stock_minimum          10000 non-null  int64         \n",
      " 15  nom_fournisseur        10000 non-null  object        \n",
      " 16  ville                  10000 non-null  object        \n",
      " 17  pays                   10000 non-null  object        \n",
      " 18  fiabilité              10000 non-null  float64       \n",
      " 19  délai_moyen_jours      10000 non-null  int64         \n",
      " 20  niveau_stock           10000 non-null  int64         \n",
      " 21  entrepot               10000 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(4), int64(8), object(9)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_global.describe()\n",
    "df_global[\"date_achat\"]=pd.to_datetime(df_global[\"date_achat\"])\n",
    "df_global.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe446f3",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda9bbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "niveau_stock",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "quantité",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stock_minimum",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "27a7bcd6-67c6-4657-b200-45278fd525a6",
       "rows": [
        [
         "count",
         "10000.0",
         "10000.0",
         "10000.0"
        ],
        [
         "mean",
         "147.7713",
         "99.5125",
         "53.283"
        ],
        [
         "std",
         "88.3549616956268",
         "98.58053385895273",
         "25.376136542637493"
        ],
        [
         "min",
         "0.0",
         "10.0",
         "10.0"
        ],
        [
         "25%",
         "72.0",
         "25.0",
         "31.0"
        ],
        [
         "50%",
         "145.0",
         "75.0",
         "54.0"
        ],
        [
         "75%",
         "226.0",
         "100.0",
         "74.0"
        ],
        [
         "max",
         "299.0",
         "500.0",
         "99.0"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>niveau_stock</th>\n",
       "      <th>quantité</th>\n",
       "      <th>stock_minimum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>147.771300</td>\n",
       "      <td>99.512500</td>\n",
       "      <td>53.283000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>88.354962</td>\n",
       "      <td>98.580534</td>\n",
       "      <td>25.376137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>72.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>145.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>54.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>226.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>74.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>299.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       niveau_stock      quantité  stock_minimum\n",
       "count  10000.000000  10000.000000   10000.000000\n",
       "mean     147.771300     99.512500      53.283000\n",
       "std       88.354962     98.580534      25.376137\n",
       "min        0.000000     10.000000      10.000000\n",
       "25%       72.000000     25.000000      31.000000\n",
       "50%      145.000000     75.000000      54.000000\n",
       "75%      226.000000    100.000000      74.000000\n",
       "max      299.000000    500.000000      99.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_global[[\"niveau_stock\",\"quantité\",\"stock_minimum\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4cb94e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération de la variable cible 'niveau_stock' (catégorielle)\n",
    "bins = [0, 1.5, 3, float('inf')]\n",
    "labels = ['bas', 'moyen', 'élevé']\n",
    "# Ratio stock actuel (quantité) par rapport au stock minimum\n",
    "# (simplification, dans la réalité le stock actuel serait une autre colonne)\n",
    "df_global['ratio_stock'] = df_global['niveau_stock'] / df_global['stock_minimum']\n",
    "df_global['niveau_stock_cat'] = pd.cut(df_global['ratio_stock'], bins=bins, labels=labels, right=False)\n",
    "# Génération de la variable cible 'rupture' (booléen)\n",
    "# Si niveau_stock est 'bas' et délai de livraison élevé -> risque de rupture\n",
    "df_global['rupture'] = ((df_global['niveau_stock_cat'] == 'bas') & (df_global['délai_livraison_jours'] > 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328a30c1",
   "metadata": {},
   "source": [
    "***explication des modifications apportées***\n",
    "- Changement des bornes des catégories de niveau de stock pour mieux refléter la distribution des données.\n",
    "- Activation de la création de la colonne 'niveau_stock_cat' pour catégoriser le niveau de stock.\n",
    "- Activation de la création de la colonne 'rupture' pour indiquer les ruptures de stock potentielles.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "811dbfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour la segmentation fournisseur, ajoutons 'taille_fournisseur'\n",
    "taille_fourn = df_global.groupby('id_fournisseur')['id_produit'].nunique().rename('taille_fournisseur_nb_produits')\n",
    "df_global = df_global.merge(taille_fourn, on='id_fournisseur', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "455fc99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour la classification de fiabilité fournisseur, créons une catégorie\n",
    "# Si fiabilité (score continu) > 0.85 -> fiable, sinon -> moins_fiable\n",
    "df_global['fiabilité_catégorie'] = pd.cut(df_global['fiabilité'], bins=[0, 0.85, 1.01], labels=['moins_fiable', 'fiable'], right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c236daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fonctions Utilitaires (pour éviter la répétition) ---\n",
    "def get_preprocessor(numerical_features, categorical_features):\n",
    "    \"\"\"Crée un ColumnTransformer pour le prétraitement.\"\"\"\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ], remainder='passthrough' # pour les colonnes non spécifiées (si besoin)\n",
    "    )\n",
    "    return preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "530afcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tâche 1: Régression du montant_total ---\n"
     ]
    }
   ],
   "source": [
    "# --- Tâche 1: Régression du montant total ---\n",
    "print(\"\\n--- Tâche 1: Régression du montant_total ---\")\n",
    "df_task1 = df_global.copy()\n",
    "X1_cols_num = ['quantité', 'prix_unitaire', 'délai_livraison_jours']\n",
    "X1_cols_cat = ['id_produit', 'catégorie', 'marque', 'id_fournisseur', 'ville', 'pays']\n",
    "y1_col = 'montant_total'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c1acafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df_task1[X1_cols_num + X1_cols_cat]\n",
    "y1 = df_task1[y1_col]\n",
    "\n",
    "# Séparation des données\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prétraitement\n",
    "preprocessor1 = get_preprocessor(X1_cols_num, X1_cols_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96beded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèles\n",
    "models1 = {\n",
    "    \"Régression Linéaire\": LinearRegression(),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(random_state=42),\n",
    "    \"XGBoost Regressor\": xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "241b8e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entraînement du modèle: Régression Linéaire\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats pour Régression Linéaire:\n",
      "  Mean Squared Error: 82158977.18\n",
      "  R² Score: 0.79\n",
      "\n",
      "Entraînement du modèle: Random Forest Regressor\n",
      "Résultats pour Random Forest Regressor:\n",
      "  Mean Squared Error: 14347.04\n",
      "  R² Score: 1.00\n",
      "\n",
      "Entraînement du modèle: XGBoost Regressor\n",
      "Résultats pour XGBoost Regressor:\n",
      "  Mean Squared Error: 158505.45\n",
      "  R² Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "for name, model in models1.items():\n",
    "    print(f\"\\nEntraînement du modèle: {name}\")\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor1), ('regressor', model)])\n",
    "    pipeline.fit(X1_train, y1_train)\n",
    "    y1_pred = pipeline.predict(X1_test)\n",
    "    \n",
    "    mse = mean_squared_error(y1_test, y1_pred)\n",
    "    r2 = r2_score(y1_test, y1_pred)\n",
    "    print(f\"Résultats pour {name}:\")\n",
    "    print(f\"  Mean Squared Error: {mse:.2f}\")\n",
    "    print(f\"  R² Score: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e89057cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tâche 2: Régression du délai_livraison_jours ---\n",
      "Résultats pour Random Forest Regressor (Délai livraison):\n",
      "  Mean Squared Error: 17.22\n",
      "  R² Score: -0.22\n"
     ]
    }
   ],
   "source": [
    "# --- Tâche 2: Régression du délai de livraison ---\n",
    "print(\"\\n--- Tâche 2: Régression du délai_livraison_jours ---\")\n",
    "df_task2 = df_global.copy()\n",
    "X2_cols_num = ['fiabilité', 'délai_moyen_jours', 'quantité', 'stock_minimum']\n",
    "X2_cols_cat = ['id_fournisseur', 'ville', 'pays', 'entrepot']\n",
    "y2_col = 'délai_livraison_jours'\n",
    "\n",
    "X2 = df_task2[X2_cols_num + X2_cols_cat]\n",
    "y2 = df_task2[y2_col]\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "preprocessor2 = get_preprocessor(X2_cols_num, X2_cols_cat)\n",
    "\n",
    "# Utilisons Random Forest pour cet exemple\n",
    "model2 = RandomForestRegressor(random_state=42)\n",
    "pipeline2 = Pipeline(steps=[('preprocessor', preprocessor2), ('regressor', model2)])\n",
    "pipeline2.fit(X2_train, y2_train)\n",
    "y2_pred = pipeline2.predict(X2_test)\n",
    "\n",
    "mse2 = mean_squared_error(y2_test, y2_pred)\n",
    "r2_2 = r2_score(y2_test, y2_pred)\n",
    "print(f\"Résultats pour Random Forest Regressor (Délai livraison):\")\n",
    "print(f\"  Mean Squared Error: {mse2:.2f}\")\n",
    "print(f\"  R² Score: {r2_2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43b71da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 27 columns):\n",
      " #   Column                          Non-Null Count  Dtype         \n",
      "---  ------                          --------------  -----         \n",
      " 0   id_achat                        10000 non-null  object        \n",
      " 1   date_achat                      10000 non-null  datetime64[ns]\n",
      " 2   id_produit                      10000 non-null  object        \n",
      " 3   quantité                        10000 non-null  int64         \n",
      " 4   id_fournisseur                  10000 non-null  object        \n",
      " 5   prix_unitaire                   10000 non-null  float64       \n",
      " 6   délai_livraison_jours           10000 non-null  int64         \n",
      " 7   montant_total                   10000 non-null  float64       \n",
      " 8   mois                            10000 non-null  int64         \n",
      " 9   année                           10000 non-null  int64         \n",
      " 10  jour_semaine                    10000 non-null  int64         \n",
      " 11  catégorie                       10000 non-null  object        \n",
      " 12  marque                          10000 non-null  object        \n",
      " 13  prix                            10000 non-null  float64       \n",
      " 14  stock_minimum                   10000 non-null  int64         \n",
      " 15  nom_fournisseur                 10000 non-null  object        \n",
      " 16  ville                           10000 non-null  object        \n",
      " 17  pays                            10000 non-null  object        \n",
      " 18  fiabilité                       10000 non-null  float64       \n",
      " 19  délai_moyen_jours               10000 non-null  int64         \n",
      " 20  niveau_stock                    10000 non-null  int64         \n",
      " 21  entrepot                        10000 non-null  object        \n",
      " 22  ratio_stock                     10000 non-null  float64       \n",
      " 23  niveau_stock_cat                10000 non-null  category      \n",
      " 24  rupture                         10000 non-null  bool          \n",
      " 25  taille_fournisseur_nb_produits  10000 non-null  int64         \n",
      " 26  fiabilité_catégorie             10000 non-null  category      \n",
      "dtypes: bool(1), category(2), datetime64[ns](1), float64(5), int64(9), object(9)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_global.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4832cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tâche 3: Classification du niveau_stock ---\n",
      "\n",
      "Entraînement du modèle: Logistic Regression\n",
      "Résultats pour Logistic Regression:\n",
      "  Accuracy: 0.27\n",
      "  Classification Report:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.int64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  Classification Report:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Afficher les noms des classes au lieu des entiers encodés\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my3_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my3_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Djo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Djo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2724\u001b[39m, in \u001b[36mclassification_report\u001b[39m\u001b[34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[39m\n\u001b[32m   2722\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2723\u001b[39m     longest_last_line_heading = \u001b[33m\"\u001b[39m\u001b[33mweighted avg\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2724\u001b[39m     name_width = \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2725\u001b[39m     width = \u001b[38;5;28mmax\u001b[39m(name_width, \u001b[38;5;28mlen\u001b[39m(longest_last_line_heading), digits)\n\u001b[32m   2726\u001b[39m     head_fmt = \u001b[33m\"\u001b[39m\u001b[33m{\u001b[39m\u001b[33m:>\u001b[39m\u001b[38;5;132;01m{width}\u001b[39;00m\u001b[33ms} \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{:>9}\u001b[39;00m\u001b[33m\"\u001b[39m * \u001b[38;5;28mlen\u001b[39m(headers)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Djo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2724\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   2722\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2723\u001b[39m     longest_last_line_heading = \u001b[33m\"\u001b[39m\u001b[33mweighted avg\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2724\u001b[39m     name_width = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcn\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m cn \u001b[38;5;129;01min\u001b[39;00m target_names)\n\u001b[32m   2725\u001b[39m     width = \u001b[38;5;28mmax\u001b[39m(name_width, \u001b[38;5;28mlen\u001b[39m(longest_last_line_heading), digits)\n\u001b[32m   2726\u001b[39m     head_fmt = \u001b[33m\"\u001b[39m\u001b[33m{\u001b[39m\u001b[33m:>\u001b[39m\u001b[38;5;132;01m{width}\u001b[39;00m\u001b[33ms} \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{:>9}\u001b[39;00m\u001b[33m\"\u001b[39m * \u001b[38;5;28mlen\u001b[39m(headers)\n",
      "\u001b[31mTypeError\u001b[39m: object of type 'numpy.int64' has no len()"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Tâche 3: Classification du niveau de stock ---\n",
    "print(\"\\n--- Tâche 3: Classification du niveau_stock ---\")\n",
    "df_task3 = df_global.copy().dropna(subset=['niveau_stock_cat']) # S'assurer qu'il n'y a pas de NaN dans la cible\n",
    "X3_cols_num = ['quantité', 'stock_minimum', 'délai_livraison_jours', 'délai_moyen_jours']\n",
    "X3_cols_cat = ['catégorie', 'marque', 'entrepot']\n",
    "y3_col = 'niveau_stock_cat'\n",
    "\n",
    "X3 = df_task3[X3_cols_num + X3_cols_cat]\n",
    "y3 = df_task3[y3_col]\n",
    "\n",
    "# Encodage de la variable cible si elle est textuelle\n",
    "le = LabelEncoder()\n",
    "y3_encoded = le.fit_transform(y3)\n",
    "\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3_encoded, test_size=0.2, random_state=42, stratify=y3_encoded)\n",
    "preprocessor3 = get_preprocessor(X3_cols_num, X3_cols_cat)\n",
    "\n",
    "models3 = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Random Forest Classifier\": RandomForestClassifier(random_state=42),\n",
    "    \"SVM Classifier\": SVC(random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models3.items():\n",
    "    print(f\"\\nEntraînement du modèle: {name}\")\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor3), ('classifier', model)])\n",
    "    pipeline.fit(X3_train, y3_train)\n",
    "    y3_pred = pipeline.predict(X3_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y3_test, y3_pred)\n",
    "    print(f\"Résultats pour {name}:\")\n",
    "    print(f\"  Accuracy: {accuracy:.2f}\")\n",
    "    print(\"  Classification Report:\")\n",
    "    # Afficher les noms des classes au lieu des entiers encodés\n",
    "    print(classification_report(y3_test, y3_pred, target_names=le.classes_, zero_division=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2c07a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tâche 4: Prédiction de rupture_de_stock ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'rupture'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Djo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'rupture'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m y4_col = \u001b[33m'\u001b[39m\u001b[33mrupture\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;66;03m# booléen (0 ou 1)\u001b[39;00m\n\u001b[32m     13\u001b[39m X4 = df_task4[X4_cols_num + X4_cols_cat]\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m y4 = \u001b[43mdf_task4\u001b[49m\u001b[43m[\u001b[49m\u001b[43my4_col\u001b[49m\u001b[43m]\u001b[49m.astype(\u001b[38;5;28mint\u001b[39m) \u001b[38;5;66;03m# Conversion booléen vers int\u001b[39;00m\n\u001b[32m     16\u001b[39m X4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m, stratify=y4)\n\u001b[32m     17\u001b[39m preprocessor4 = get_preprocessor(X4_cols_num, X4_cols_cat)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Djo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Djo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'rupture'"
     ]
    }
   ],
   "source": [
    "# --- Tâche 4: Prédiction de rupture de stock ---\n",
    "print(\"\\n--- Tâche 4: Prédiction de rupture_de_stock ---\")\n",
    "df_task4 = df_global.copy()\n",
    "# Ajout des features temporelles si 'date_achat' est disponible\n",
    "df_task4['mois'] = df_task4['date_achat'].dt.month\n",
    "df_task4['année'] = df_task4['date_achat'].dt.year\n",
    "df_task4['jour_semaine'] = df_task4['date_achat'].dt.dayofweek\n",
    "\n",
    "X4_cols_num = ['stock_minimum', 'quantité', 'délai_moyen_jours', 'mois', 'année', 'jour_semaine']\n",
    "X4_cols_cat = ['niveau_stock', 'entrepot'] # niveau_stock est déjà catégoriel\n",
    "y4_col = 'rupture' # booléen (0 ou 1)\n",
    "\n",
    "X4 = df_task4[X4_cols_num + X4_cols_cat]\n",
    "y4 = df_task4[y4_col].astype(int) # Conversion booléen vers int\n",
    "\n",
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, test_size=0.2, random_state=42, stratify=y4)\n",
    "preprocessor4 = get_preprocessor(X4_cols_num, X4_cols_cat)\n",
    "\n",
    "model4 = RandomForestClassifier(random_state=42)\n",
    "pipeline4 = Pipeline(steps=[('preprocessor', preprocessor4), ('classifier', model4)])\n",
    "pipeline4.fit(X4_train, y4_train)\n",
    "y4_pred = pipeline4.predict(X4_test)\n",
    "\n",
    "accuracy4 = accuracy_score(y4_test, y4_pred)\n",
    "print(f\"Résultats pour Random Forest Classifier (Rupture de stock):\")\n",
    "print(f\"  Accuracy: {accuracy4:.2f}\")\n",
    "print(\"  Classification Report:\")\n",
    "print(classification_report(y4_test, y4_pred, zero_division=0))\n",
    "print(\"  Confusion Matrix:\")\n",
    "print(confusion_matrix(y4_test, y4_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53100ee2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3884377133.py, line 203)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 203\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mUse code with caution.\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Tâche 5: Prévision de la demande (séries temporelles) ---\n",
    "print(\"\\n--- Tâche 5: Prévision de la demande (quantité par produit) ---\")\n",
    "# Exemple avec UN SEUL produit pour simplifier. En pratique, on ferait une boucle ou un modèle par produit.\n",
    "df_task5_prodA = df_global[df_global['id_produit'] == 'PROD_A'].copy()\n",
    "df_task5_prodA = df_task5_prodA.set_index('date_achat')\n",
    "# Agréger par mois pour avoir une série plus lisse (optionnel)\n",
    "ts_data = df_task5_prodA['quantité'].resample('M').sum()\n",
    "\n",
    "if len(ts_data) > 12 : # Besoin d'assez de données pour Prophet/ARIMA\n",
    "    # Séparation Temporelle\n",
    "    train_size = int(len(ts_data) * 0.8)\n",
    "    ts_train, ts_test = ts_data[0:train_size], ts_data[train_size:]\n",
    "\n",
    "    # Modèle Prophet (souvent plus simple à mettre en oeuvre initialement)\n",
    "    print(\"\\nModèle Prophet:\")\n",
    "    df_prophet_train = ts_train.reset_index()\n",
    "    df_prophet_train.columns = ['ds', 'y'] # Prophet requiert ces noms de colonnes\n",
    "\n",
    "    model_prophet = Prophet()\n",
    "    model_prophet.fit(df_prophet_train)\n",
    "\n",
    "    future_dates = model_prophet.make_future_dataframe(periods=len(ts_test), freq='M')\n",
    "    forecast_prophet = model_prophet.predict(future_dates)\n",
    "    \n",
    "    y5_pred_prophet = forecast_prophet['yhat'][-len(ts_test):].values # Prendre les prédictions pour la période de test\n",
    "\n",
    "    # Évaluation Prophet\n",
    "    # Note: L'évaluation directe sur ts_test peut nécessiter un alignement d'index\n",
    "    if len(y5_pred_prophet) == len(ts_test):\n",
    "        mse_prophet = mean_squared_error(ts_test.values, y5_pred_prophet)\n",
    "        print(f\"  MSE Prophet: {mse_prophet:.2f}\")\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(ts_train.index, ts_train.values, label='Train')\n",
    "        plt.plot(ts_test.index, ts_test.values, label='Test Réel')\n",
    "        plt.plot(ts_test.index, y5_pred_prophet, label='Forecast Prophet')\n",
    "        plt.title('Prévision de la demande (PROD_A) avec Prophet')\n",
    "        plt.legend()\n",
    "        # plt.show() # Décommenter pour afficher le graphique\n",
    "        print(\"Graphique Prophet généré (décommentez plt.show() pour l'afficher).\")\n",
    "    else:\n",
    "        print(\"  Problème de dimension pour l'évaluation Prophet.\")\n",
    "        \n",
    "    # Modèle ARIMA (exemple basique)\n",
    "    # L'analyse ARIMA nécessite des étapes de vérification de stationnarité, détermination des ordres p,d,q (ACF/PACF)\n",
    "    # Ce qui est hors de portée d'un script automatisé simple.\n",
    "    # Voici un exemple avec des ordres arbitraires (à ajuster après analyse)\n",
    "    print(\"\\nModèle ARIMA (exemple basique):\")\n",
    "    try:\n",
    "        model_arima = ARIMA(ts_train, order=(5,1,0)) # Ordres (p,d,q) à déterminer\n",
    "        model_arima_fit = model_arima.fit()\n",
    "        y5_pred_arima = model_arima_fit.forecast(steps=len(ts_test))\n",
    "\n",
    "        mse_arima = mean_squared_error(ts_test.values, y5_pred_arima)\n",
    "        print(f\"  MSE ARIMA: {mse_arima:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Erreur ARIMA: {e}\")\n",
    "else:\n",
    "    print(\"Pas assez de données pour la série temporelle du produit PROD_A.\")\n",
    "\n",
    "\n",
    "# --- Tâche 6: Segmentation produits ---\n",
    "print(\"\\n--- Tâche 6: Segmentation produits ---\")\n",
    "df_task6 = df_global.copy()\n",
    "# Utiliser des moyennes par produit ou des caractéristiques statiques\n",
    "# Ici, nous allons prendre la moyenne des caractéristiques numériques par produit\n",
    "# et les caractéristiques catégorielles les plus fréquentes (ou les considérer comme des features globales du produit)\n",
    "# Pour simplifier, prenons quelques caractéristiques et faisons une segmentation sur les produits uniques.\n",
    "# Pour une vraie segmentation, on prendrait des caractéristiques agrégées par produit\n",
    "# Ex: prix moyen, catégorie principale, marque principale, stock_minimum moyen, etc.\n",
    "prod_features = df_task6.groupby('id_produit').agg(\n",
    "    prix_moyen=('prix_unitaire', 'mean'),\n",
    "    stock_min_moyen=('stock_minimum', 'mean'),\n",
    "    delai_liv_moyen=('délai_livraison_jours', 'mean'),\n",
    "    catégorie_mode=('catégorie', lambda x: x.mode()[0] if not x.mode().empty else 'N/A'), # Mode pour la catégorie\n",
    "    marque_mode=('marque', lambda x: x.mode()[0] if not x.mode().empty else 'N/A') # Mode pour la marque\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "X6_cols_num = ['prix_moyen', 'stock_min_moyen', 'delai_liv_moyen']\n",
    "X6_cols_cat = ['catégorie_mode', 'marque_mode'] # ces colonnes sont déjà au niveau produit\n",
    "X6_processed = prod_features.copy() # Pour garder une trace des id_produit\n",
    "\n",
    "# Prétraitement\n",
    "# Les colonnes catégorielles sont déjà 'agrégées', on peut les one-hot encoder directement\n",
    "# Création du preprocessor\n",
    "preprocessor6 = get_preprocessor(X6_cols_num, X6_cols_cat)\n",
    "X6_prepared = preprocessor6.fit_transform(X6_processed[X6_cols_num + X6_cols_cat])\n",
    "\n",
    "if X6_prepared.shape[0] > 1 : # K-means a besoin d'au moins n_clusters échantillons\n",
    "    # K-Means\n",
    "    # Déterminer le nombre optimal de clusters (méthode du coude - non implémentée ici pour brièveté)\n",
    "    n_clusters_prod = min(3, X6_prepared.shape[0]) # Exemple: 3 clusters ou moins si peu de produits\n",
    "    if n_clusters_prod > 1:\n",
    "        kmeans_prod = KMeans(n_clusters=n_clusters_prod, random_state=42, n_init='auto')\n",
    "        prod_features['cluster_kmeans'] = kmeans_prod.fit_predict(X6_prepared)\n",
    "        \n",
    "        silhouette_kmeans = silhouette_score(X6_prepared, prod_features['cluster_kmeans'])\n",
    "        print(f\"K-Means (Produits) Silhouette Score: {silhouette_kmeans:.2f}\")\n",
    "        print(\"Premiers produits avec leurs clusters K-Means:\")\n",
    "        print(prod_features[['id_produit', 'cluster_kmeans']].head())\n",
    "\n",
    "        # DBSCAN (plus robuste aux formes de clusters et outliers, mais sensible aux paramètres)\n",
    "        # Les paramètres eps et min_samples sont cruciaux et demandent une exploration\n",
    "        dbscan_prod = DBSCAN(eps=0.5, min_samples=max(1, int(X6_prepared.shape[0]*0.1))) # Exemple de paramètres\n",
    "        prod_features['cluster_dbscan'] = dbscan_prod.fit_predict(X6_prepared)\n",
    "        \n",
    "        # Le score de silhouette n'est pas toujours pertinent pour DBSCAN s'il y a des outliers (-1)\n",
    "        # On ne calcule que si plus d'un cluster est trouvé (excluant les outliers)\n",
    "        valid_clusters_dbscan = prod_features['cluster_dbscan'][prod_features['cluster_dbscan'] != -1]\n",
    "        if len(np.unique(valid_clusters_dbscan)) > 1:\n",
    "            silhouette_dbscan = silhouette_score(X6_prepared[prod_features['cluster_dbscan'] != -1], valid_clusters_dbscan)\n",
    "            print(f\"DBSCAN (Produits) Silhouette Score (sans outliers): {silhouette_dbscan:.2f}\")\n",
    "        print(\"Premiers produits avec leurs clusters DBSCAN:\")\n",
    "        print(prod_features[['id_produit', 'cluster_dbscan']].head())\n",
    "    else:\n",
    "        print(\"Pas assez de clusters potentiels pour la segmentation des produits.\")\n",
    "else:\n",
    "    print(\"Pas assez de produits uniques pour la segmentation.\")\n",
    "\n",
    "\n",
    "# --- Tâche 7: Segmentation fournisseurs ---\n",
    "print(\"\\n--- Tâche 7: Segmentation fournisseurs ---\")\n",
    "df_task7 = df_global.copy()\n",
    "# Agréger les données par fournisseur\n",
    "fourn_features = df_task7.groupby('id_fournisseur').agg(\n",
    "    fiabilité_moyenne=('fiabilité', 'mean'),\n",
    "    delai_moyen_global=('délai_moyen_jours', 'mean'), # Déjà un délai moyen, on pourrait prendre la moyenne des delais de livraison effectifs\n",
    "    ville_mode=('ville', lambda x: x.mode()[0] if not x.mode().empty else 'N/A'),\n",
    "    pays_mode=('pays', lambda x: x.mode()[0] if not x.mode().empty else 'N/A'),\n",
    "    taille_fournisseur=('taille_fournisseur_nb_produits', 'first') # Déjà calculé\n",
    ").reset_index()\n",
    "\n",
    "X7_cols_num = ['fiabilité_moyenne', 'delai_moyen_global', 'taille_fournisseur']\n",
    "X7_cols_cat = ['ville_mode', 'pays_mode']\n",
    "X7_processed = fourn_features.copy()\n",
    "\n",
    "preprocessor7 = get_preprocessor(X7_cols_num, X7_cols_cat)\n",
    "X7_prepared = preprocessor7.fit_transform(X7_processed[X7_cols_num + X7_cols_cat])\n",
    "\n",
    "if X7_prepared.shape[0] > 1:\n",
    "    n_clusters_fourn = min(3, X7_prepared.shape[0]) # Exemple: 3 clusters\n",
    "    if n_clusters_fourn > 1:\n",
    "        kmeans_fourn = KMeans(n_clusters=n_clusters_fourn, random_state=42, n_init='auto')\n",
    "        fourn_features['cluster_kmeans'] = kmeans_fourn.fit_predict(X7_prepared)\n",
    "        \n",
    "        silhouette_kmeans_f = silhouette_score(X7_prepared, fourn_features['cluster_kmeans'])\n",
    "        print(f\"K-Means (Fournisseurs) Silhouette Score: {silhouette_kmeans_f:.2f}\")\n",
    "        print(\"Premiers fournisseurs avec leurs clusters K-Means:\")\n",
    "        print(fourn_features[['id_fournisseur', 'cluster_kmeans']].head())\n",
    "    else:\n",
    "        print(\"Pas assez de clusters potentiels pour la segmentation des fournisseurs.\")\n",
    "else:\n",
    "    print(\"Pas assez de fournisseurs uniques pour la segmentation.\")\n",
    "\n",
    "\n",
    "# --- Tâche 8: Classification du fournisseur fiable ---\n",
    "print(\"\\n--- Tâche 8: Classification du fournisseur fiable ---\")\n",
    "df_task8 = df_global.copy().dropna(subset=['fiabilité_catégorie'])\n",
    "# Agréger les données par fournisseur pour la classification\n",
    "fourn_class_features = df_task8.groupby('id_fournisseur').agg(\n",
    "    delai_moyen_liv=('délai_livraison_jours', 'mean'),\n",
    "    montant_total_moyen=('montant_total', 'mean'),\n",
    "    quantite_moyenne=('quantité', 'mean'),\n",
    "    nb_ruptures=('rupture', 'sum'), # Nombre de fois où ce fournisseur a été associé à une rupture\n",
    "    fiabilité_cible=('fiabilité_catégorie', 'first') # Cible\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "X8_cols_num = ['delai_moyen_liv', 'montant_total_moyen', 'quantite_moyenne', 'nb_ruptures']\n",
    "# Pas de X8_cols_cat dans cet exemple agrégé, mais on pourrait en ajouter\n",
    "y8_col = 'fiabilité_cible'\n",
    "\n",
    "X8 = fourn_class_features[X8_cols_num] # S'il y avait des X8_cols_cat, les ajouter ici\n",
    "y8 = fourn_class_features[y8_col]\n",
    "\n",
    "le8 = LabelEncoder()\n",
    "y8_encoded = le8.fit_transform(y8)\n",
    "\n",
    "if len(np.unique(y8_encoded)) > 1: # S'assurer qu'il y a plus d'une classe\n",
    "    X8_train, X8_test, y8_train, y8_test = train_test_split(X8, y8_encoded, test_size=0.2, random_state=42, stratify=y8_encoded)\n",
    "    \n",
    "    # Prétraitement simplifié car que des numériques ici (à adapter si catégorielles)\n",
    "    preprocessor8_num_only = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    model8 = RandomForestClassifier(random_state=42)\n",
    "    pipeline8 = Pipeline(steps=[('preprocessor', preprocessor8_num_only), ('classifier', model8)])\n",
    "    pipeline8.fit(X8_train, y8_train)\n",
    "    y8_pred = pipeline8.predict(X8_test)\n",
    "\n",
    "    accuracy8 = accuracy_score(y8_test, y8_pred)\n",
    "    print(f\"Résultats pour Random Forest Classifier (Fiabilité fournisseur):\")\n",
    "    print(f\"  Accuracy: {accuracy8:.2f}\")\n",
    "    print(\"  Classification Report:\")\n",
    "    print(classification_report(y8_test, y8_pred, target_names=le8.classes_, zero_division=0))\n",
    "else:\n",
    "    print(\"Pas assez de classes différentes pour la classification de fiabilité fournisseur.\")\n",
    "\n",
    "print(\"\\n--- Fin des Tâches ---\")\n",
    "Use code with caution.\n",
    "Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2798bbca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726f0516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558b17ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
