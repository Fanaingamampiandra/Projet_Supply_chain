{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48eaf2c9",
   "metadata": {},
   "source": [
    "# ABC analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d7b8a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('../base/achatsV2.csv',sep=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9161396d",
   "metadata": {},
   "source": [
    "Pour chaque id_produit, tu dois calculer :\n",
    "\n",
    "La moyenne des quantités demandées (ou vendues)\n",
    "\n",
    "L’écart-type\n",
    "\n",
    "Puis le coefficient de variation (CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaf6ee45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id_achat', 'date_achat', 'id_produit', 'quantité', 'id_fournisseur',\n",
      "       'prix_unitaire', 'délai_livraison_jours'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Vérifie les colonnes disponibles\n",
    "print(df.columns)\n",
    "\n",
    "# Exemple : suppose que les colonnes utiles sont 'id_produit' et 'quantite'\n",
    "# Calcule le CV par produit\n",
    "cv_df = df.groupby('id_produit')['quantité'].agg(['mean', 'std'])\n",
    "cv_df['CV'] = cv_df['std'] / cv_df['mean']\n",
    "cv_df = cv_df.reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3af7e57",
   "metadata": {},
   "source": [
    "**2. Classifier la variabilité : Haute ou Basse**\n",
    "Utilise un seuil sur le CV, par exemple :\n",
    "\n",
    "CV > 1 → Haute variabilité\n",
    "\n",
    "CV ≤ 1 → Faible variabilité\n",
    "\n",
    "python\n",
    "Copier\n",
    "Modifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df['variabilite'] = cv_df['CV'].apply(lambda x: 'HIGH' if x > 1 else 'LOW')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388342ae",
   "metadata": {},
   "source": [
    "3. Calculer la valeur économique des produits\n",
    "Utilise la valeur d’achat totale par produit (ou un autre indicateur économique pertinent comme CA, marge, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3ccd280",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'quantite'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Djo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'quantite'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Suppose qu'on a les colonnes 'prix_unitaire' et 'quantite'\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mvaleur_totale\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquantite\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m * df[\u001b[33m'\u001b[39m\u001b[33mprix_unitaire\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Total économique par produit\u001b[39;00m\n\u001b[32m      5\u001b[39m valeurs = df.groupby(\u001b[33m'\u001b[39m\u001b[33mid_produit\u001b[39m\u001b[33m'\u001b[39m)[\u001b[33m'\u001b[39m\u001b[33mvaleur_totale\u001b[39m\u001b[33m'\u001b[39m].sum().reset_index()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Djo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Djo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'quantite'"
     ]
    }
   ],
   "source": [
    "# Suppose qu'on a les colonnes 'prix_unitaire' et 'quantite'\n",
    "df['valeur_totale'] = df['quantite'] * df['prix_unitaire']\n",
    "\n",
    "# Total économique par produit\n",
    "valeurs = df.groupby('id_produit')['valeur_totale'].sum().reset_index()\n",
    "valeurs = valeurs.sort_values('valeur_totale', ascending=False)\n",
    "\n",
    "# Classe ABC (selon 80/15/5 % de la valeur cumulée)\n",
    "valeurs['part_cumulee'] = valeurs['valeur_totale'].cumsum() / valeurs['valeur_totale'].sum()\n",
    "\n",
    "def get_classe(part):\n",
    "    if part <= 0.8:\n",
    "        return 'A'\n",
    "    elif part <= 0.95:\n",
    "        return 'B'\n",
    "    else:\n",
    "        return 'C'\n",
    "\n",
    "valeurs['classe_economique'] = valeurs['part_cumulee'].apply(get_classe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96bed0d",
   "metadata": {},
   "source": [
    "4. Fusionner les deux classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fa93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusion des deux tableaux\n",
    "segmentation = pd.merge(cv_df[['id_produit', 'variabilite']], valeurs[['id_produit', 'classe_economique']], on='id_produit')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff95650",
   "metadata": {},
   "source": [
    "5. Ajouter la segmentation finale (3 zones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf4e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment(row):\n",
    "    if row['variabilite'] == 'HIGH' and row['classe_economique'] in ['A', 'B']:\n",
    "        return 'HIGH IMPORTANCE'\n",
    "    elif row['variabilite'] == 'LOW' and row['classe_economique'] in ['A', 'B']:\n",
    "        return 'STABLE DEMAND'\n",
    "    else:\n",
    "        return 'LOW IMPORTANCE'\n",
    "\n",
    "segmentation['segment'] = segmentation.apply(get_segment, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17848d47",
   "metadata": {},
   "source": [
    "## MACHINE LEARNING achat_produit_fournisseur_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e355bf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec82e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Chargement des données\n",
    "df = pd.read_csv(\"acaht_prod_fourn_stock.csv\")\n",
    "\n",
    "df['date_achat'] = pd.to_datetime(df['date_achat'])\n",
    "\n",
    "# Création de caractéristiques temporelles\n",
    "df['mois_achat'] = df['date_achat'].dt.month\n",
    "df['annee'] = df['date_achat'].dt.year\n",
    "df['jour_semaine'] = df['date_achat'].dt.dayofweek\n",
    "df['trimestre'] = df['date_achat'].dt.quarter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7026dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417ee9b7",
   "metadata": {},
   "source": [
    "### Préprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2374b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identification des variables numériques et catégorielles\n",
    "cat_features = ['id_produit', 'id_fournisseur', 'catégorie', 'marque', \n",
    "                'nom_fournisseur', 'ville', 'pays', 'entrepot']\n",
    "num_features = ['prix_unitaire', 'fiabilité', 'stock_minimum', 'niveau_stock', \n",
    "                'délai_moyen_jours', 'prix']\n",
    "\n",
    "# Préprocesseur pour transformer les variables\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3face0",
   "metadata": {},
   "source": [
    "### Modèle de prévision de la demande\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fe65a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrégation des données par produit et par mois pour la prévision\n",
    "demand_data = df.groupby(['id_produit', 'année', 'mois'])['quantité'].sum().reset_index()\n",
    "\n",
    "# Création de variables lag (historique des ventes précédentes)\n",
    "for lag in [1, 2, 3, 6]:\n",
    "    demand_data[f'lag_{lag}'] = demand_data.groupby('id_produit')['quantité'].shift(lag)\n",
    "\n",
    "# Suppression des lignes avec valeurs manquantes (dues aux lags)\n",
    "demand_data = demand_data.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06428759",
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84c89ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identification des variables numériques et catégorielles\n",
    "cat_features_ = ['id_produit']\n",
    "num_features_ = ['mois', 'lag_1', 'lag_2', 'lag_3', 'lag_6']\n",
    "\n",
    "preprocessor1 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_features_),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features_)\n",
    "    ])\n",
    "\n",
    "# Préparation des features et de la cible\n",
    "X = demand_data.drop(['quantité'], axis=1)\n",
    "y = demand_data['quantité']\n",
    "\n",
    "# Division train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modèle XGBoost pour la prévision de demande\n",
    "model_demand = Pipeline([\n",
    "    ('preprocessor', preprocessor1),\n",
    "    ('regressor', XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5))\n",
    "])\n",
    "\n",
    "# Entraînement et évaluation\n",
    "model_demand.fit(X_train, y_train)\n",
    "y_pred = model_demand.predict(X_test)\n",
    "\n",
    "# Métriques d'évaluation\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred)}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred))}\")\n",
    "print(f\"R²: {r2_score(y_test, y_pred)}\")\n",
    "\n",
    "# Analyse de l'importance des features\n",
    "feature_importance = model_demand.named_steps['regressor'].feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0c4fb4",
   "metadata": {},
   "source": [
    "### Modèle d'estimation des délais fournisseurs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données pour le modèle de délai\n",
    "X_delay = df[['id_fournisseur', 'quantité', 'catégorie', 'prix_unitaire','prix','fiabilité', 'mois', 'jour_semaine', 'pays', 'ville','entrepot']]\n",
    "y_delay = df['délai_livraison_jours']\n",
    "\n",
    "# Identification des variables numériques et catégorielles\n",
    "cat_featuresf = ['id_fournisseur', 'catégorie','mois', 'jour_semaine','entrepot','ville', 'pays']\n",
    "num_featuresf = ['prix_unitaire', 'fiabilité', 'prix','quantite']\n",
    "\n",
    "# Préprocesseur pour transformer les variables\n",
    "preprocessorf = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_featuresf),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_featuresf)\n",
    "    ])\n",
    "\n",
    "# Division train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_delay, y_delay, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modèle Random Forest pour la prédiction des délais\n",
    "model_delay = Pipeline([\n",
    "    ('preprocessor', preprocessorf),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42))\n",
    "])\n",
    "\n",
    "# Entraînement et évaluation\n",
    "model_delay.fit(X_train, y_train)\n",
    "y_pred = model_delay.predict(X_test)\n",
    "\n",
    "# Métriques d'évaluation\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred)}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred))}\")\n",
    "print(f\"R²: {r2_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1612256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
    "\n",
    "# Pour le modèle de prévision de demande (validation temporelle)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "cv_scores = cross_val_score(model_demand, X, y, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "print(f\"MAE CV: {-np.mean(cv_scores)}\")\n",
    "\n",
    "# Pour le modèle de délai (validation standard)\n",
    "cv_scores = cross_val_score(model_delay, X_delay, y_delay, cv=5, scoring='neg_mean_absolute_error')\n",
    "print(f\"MAE CV: {-np.mean(cv_scores)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b212ea61",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e72faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering avancé\n",
    "df['demande_rolling_30j'] = df.groupby('id_produit')['quantité'].transform(\n",
    "    lambda x: x.rolling(window=30, min_periods=1).mean()\n",
    ")\n",
    "df['pression_stock'] = df['niveau_stock'] / df['stock_minimum']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score\n",
    "\n",
    "# 1. Chargement des données\n",
    "df = pd.read_csv('.../code/achat_prod_fournisseur_stock.csv',sep=';', parse_dates=['date_achat', 'date_expedition', 'date_livraison'])\n",
    "# Supposons une table retours dans retours.csv\n",
    "retours = pd.read_csv('.../base/retoursV2.csv', parse_dates=['date_retour'])\n",
    "\n",
    "# 2. Calcul des KPI\n",
    "# 2.1 Rupture de stock\n",
    "df['rupture_flag'] = (df['niveau_stock'] < df['stock_minimum']).astype(int)\n",
    "kpi_rupture = df.groupby('id_produit')['rupture_flag'].mean().reset_index(name='taux_rupture_pct')\n",
    "\n",
    "# 2.2 OTIF\n",
    "# OnTimeFlag et FullFlag\n",
    "df['on_time'] = (df['date_livraison'] <= df['date_attendue']).astype(int)\n",
    "df['full']    = (df['quant_livree'] >= df['quant_attendue']).astype(int)\n",
    "df['otif_flag'] = df['on_time'] * df['full']\n",
    "kpi_otif     = df['otif_flag'].mean() * 100\n",
    "\n",
    "# 2.3 Taux de retour\n",
    "ret_total = retours.groupby('id_commande')['quantite_retournee'].sum().reset_index()\n",
    "deliv_total = df.groupby('id_commande')['quant_livree'].sum().reset_index()\n",
    "ret = ret_total.merge(deliv_total, on='id_commande')\n",
    "kpi_taux_retour = (ret['quantite_retournee'].sum() / ret['quant_livree'].sum()) * 100\n",
    "\n",
    "# 2.4 Délai moyen expédition\n",
    "df['delai_jours'] = (df['date_livraison'] - df['date_expedition']).dt.days\n",
    "kpi_delai_moyen = df['delai_jours'].mean()\n",
    "\n",
    "# 2.5 Stock turnover\n",
    "# consommation annuelle et stock moyen\n",
    "consommation = df.groupby(df['date_achat'].dt.year)['quant_achetee'].sum().iloc[-1]\n",
    "stock_jan1 = df[df['date_achat'] == pd.to_datetime(f\"{df['date_achat'].dt.year.max()}-01-01\")]['niveau_stock'].mean()\n",
    "stock_dec31 = df[df['date_achat'] == pd.to_datetime(f\"{df['date_achat'].dt.year.max()}-12-31\")]['niveau_stock'].mean()\n",
    "stock_moyen = (stock_jan1 + stock_dec31) / 2\n",
    "kpi_rotation = consommation / stock_moyen\n",
    "\n",
    "# 2.6 Coût logistique unitaire\n",
    "kpi_cout_unitaire = df['cout_transport'].sum() / df['quant_livree'].sum()\n",
    "\n",
    "# 2.7 Fill Rate\n",
    "df['fill_rate'] = df['quant_livree'] / df['quant_attendue']\n",
    "kpi_fill_rate = df['fill_rate'].mean() * 100\n",
    "\n",
    "# Agrégation des KPI globaux\n",
    "glob_kpis = pd.DataFrame({\n",
    "    'otif_pct': [kpi_otif],\n",
    "    'taux_retour_pct': [kpi_taux_retour],\n",
    "    'delai_moyen_jours': [kpi_delai_moyen],\n",
    "    'rotation_stock': [kpi_rotation],\n",
    "    'cout_unitaire': [kpi_cout_unitaire],\n",
    "    'fill_rate_pct': [kpi_fill_rate]\n",
    "})\n",
    "\n",
    "# 3. Feature Engineering pour ML\n",
    "# Ajout de variables temporelles\n",
    "df['jour_semaine'] = df['date_achat'].dt.dayofweek\n",
    "df['mois']         = df['date_achat'].dt.month\n",
    "# Moyenne mobile du niveau stock\n",
    "df = df.sort_values('date_achat')\n",
    "df['stock_mm_7'] = df.groupby('id_produit')['niveau_stock'].transform(lambda x: x.rolling(7, min_periods=1).mean())\n",
    "\n",
    "# Merge kpi_rupture sur df\n",
    "df = df.merge(kpi_rupture, on='id_produit', how='left')\n",
    "\n",
    "# 4. Cas d'usage: prédiction du délai fournisseur (Régression)\n",
    "features = ['jour_semaine', 'mois', 'niv...']  # compléter\n",
    "X = df[features]\n",
    "y = df['delai_jours']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "reg.fit(X_train, y_train)\n",
    "pred = reg.predict(X_test)\n",
    "print('MAE délai:', mean_absolute_error(y_test, pred))\n",
    "\n",
    "# 5. Cas d'usage: classification retour\n",
    "# Préparation dataset retour vs non-retour\n",
    "df_class = df.merge(ret[['id_commande','quantite_retournee']], on='id_commande', how='left').fillna(0)\n",
    "df_class['retour_flag'] = (df_class['quantite_retournee']>0).astype(int)\n",
    "Xc = df_class[features]\n",
    "yc = df_class['retour_flag']\n",
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(Xc, yc, test_size=0.2, random_state=42, stratify=yc)\n",
    "clf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "clf.fit(Xc_train, yc_train)\n",
    "yc_pred = clf.predict_proba(Xc_test)[:,1]\n",
    "print('ROC AUC retour:', roc_auc_score(yc_test, yc_pred))\n",
    "\n",
    "# Export des KPI pour reporting\n",
    "glob_kpis.to_csv('kpi_supply_chain.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7470be",
   "metadata": {},
   "source": [
    "## Other model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25036f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score, roc_curve\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# 1. Chargement & préparation\n",
    "df = pd.read_csv('../stephanie/acaht_prod_fourn_stock.csv', parse_dates=['date_achat','date_expedition','date_livraison'])\n",
    "retours = pd.read_csv('../stephanie/retoursV2.csv', parse_dates=['date_retour'])\n",
    "\n",
    "# KPI calculs (cf. sections précédentes)\n",
    "df['rupture_flag'] = (df['niveau_stock'] < df['stock_minimum']).astype(int)\n",
    "df['on_time']      = (df['date_livraison'] <= df['date_attendue']).astype(int)\n",
    "df['full']         = (df['quant_livree'] >= df['quant_attendue']).astype(int)\n",
    "df['otif_flag']    = df['on_time'] * df['full']\n",
    "ret_tot   = retours.groupby('id_commande')['quantite_retournee'].sum()\n",
    "deliv_tot = df.groupby('id_commande')['quant_livree'].sum()\n",
    "ret = ret_tot.to_frame('quantite_retournee').join(deliv_tot.to_frame('quant_livree'), how='inner')\n",
    "df['delai_jours']  = (df['date_livraison'] - df['date_expedition']).dt.days\n",
    "\n",
    "glob_kpis = pd.DataFrame({\n",
    "    'otif_pct':      [df['otif_flag'].mean()*100],\n",
    "    'taux_retour_pct': [(ret['quantite_retournee'].sum()/ret['quant_livree'].sum())*100],\n",
    "    'delai_moyen_jours': [df['delai_jours'].mean()],\n",
    "    'rotation_stock': [ df.groupby(df['date_achat'].dt.year)['quant_achetee'].sum().iloc[-1] / \\\n",
    "                       ((df[df['date_achat'].dt.dayofyear==1]['niveau_stock'].mean()+\n",
    "                         df[df['date_achat'].dt.dayofyear==365]['niveau_stock'].mean())/2) ],\n",
    "    'cout_unitaire': [df['cout_transport'].sum()/df['quant_livree'].sum()],\n",
    "    'fill_rate_pct': [df['quant_livree'].sum()/df['quant_attendue'].sum()*100]\n",
    "})\n",
    "\n",
    "# 2. Feature Engineering pour ML\n",
    "df = df.sort_values('date_achat')\n",
    "df['jour_semaine'] = df['date_achat'].dt.dayofweek\n",
    "df['mois']         = df['date_achat'].dt.month\n",
    "for lag in [1,7,14]:\n",
    "    df[f'lag_{lag}'] = df.groupby('id_produit')['quant_achetee'].shift(lag)\n",
    "for w in [7,30]:\n",
    "    df[f'mm_{w}'] = df.groupby('id_produit')['quant_achetee'].transform(lambda x: x.rolling(w,1).mean())\n",
    "# suppress NaNs\n",
    "df_ml = df.dropna(subset=[f'lag_{l}' for l in [1,7,14]] + [f'mm_{w}' for w in [7,30]])\n",
    "\n",
    "# 3. Forecasting\n",
    "results = {}\n",
    "# 3.1 Série temporelle SARIMAX par produit\n",
    "prods = df_ml['id_produit'].unique()[:3]  # exemple\n",
    "for pid in prods:\n",
    "    ts = df_ml[df_ml['id_produit']==pid].set_index('date_achat')['quant_achetee']\n",
    "    train, test = ts.iloc[:-30], ts.iloc[-30:]\n",
    "    model = SARIMAX(train, order=(1,1,1), seasonal_order=(1,1,1,7))\n",
    "    fit = model.fit(disp=False)\n",
    "    pred = fit.predict(start=test.index[0], end=test.index[-1])\n",
    "    mae = mean_absolute_error(test, pred)\n",
    "    results[pid] = {'sarimax_mae': mae, 'actual':test, 'pred':pred}\n",
    "\n",
    "# 3.2 ML-based forecasting (RandomForest)\n",
    "features = ['jour_semaine','mois','lag_1','lag_7','lag_14','mm_7','mm_30']\n",
    "fore_mae = {}\n",
    "for pid in prods:\n",
    "    sub = df_ml[df_ml['id_produit']==pid]\n",
    "    X = sub[features]; y=sub['quant_achetee']\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    maes=[]\n",
    "    for tr, ts in tscv.split(X):\n",
    "        rf=RandomForestRegressor(n_estimators=100,random_state=0)\n",
    "        rf.fit(X.iloc[tr],y.iloc[tr])\n",
    "        p=rf.predict(X.iloc[ts])\n",
    "        maes.append(mean_absolute_error(y.iloc[ts],p))\n",
    "    fore_mae[pid] = np.mean(maes)\n",
    "    results[pid]['rf_mae']=fore_mae[pid]\n",
    "\n",
    "# 4. Clustering\n",
    "# 4.1 Produits\n",
    "prod_feats = df.groupby('id_produit').agg({\n",
    "    'quant_achetee':'sum','prix_unitaire':'mean','niveau_stock':'mean','delai_jours':'mean'\n",
    "}).reset_index()\n",
    "kmeans_p = KMeans(n_clusters=4, random_state=0).fit(prod_feats.drop('id_produit',1))\n",
    "prod_feats['cluster_prod']=kmeans_p.labels_\n",
    "# 4.2 Fournisseurs\n",
    "fourn_feats = df.groupby('nom_fournisseur').agg({\n",
    "    'quant_achetee':'sum','prix_unitaire':'mean','delai_jours':'mean'\n",
    "}).reset_index()\n",
    "kmeans_f = KMeans(n_clusters=3, random_state=0).fit(fourn_feats.drop('nom_fournisseur',1))\n",
    "fourn_feats['cluster_fourn']=kmeans_f.labels_\n",
    "\n",
    "# 5. Hyperparam & validation croisée (exemple RandomForest rég.)\n",
    "param_dist = {'n_estimators':[50,100,200],'max_depth':[5,10,None],'min_samples_split':[2,5,10]}\n",
    "rs = RandomizedSearchCV(RandomForestRegressor(), param_dist, n_iter=10, cv=3, scoring='neg_mean_absolute_error', random_state=0)\n",
    "X_all, y_all = df_ml[features], df_ml['delai_jours']\n",
    "rs.fit(X_all, y_all)\n",
    "best_params = rs.best_params_\n",
    "\n",
    "# 6. Visualisations\n",
    "plt.figure(); glob_kpis.T.plot(kind='bar', legend=False); plt.title('KPI globaux'); plt.tight_layout(); plt.show()\n",
    "\n",
    "for pid,res in results.items():\n",
    "    plt.figure();\n",
    "    res['actual'].plot(label='actuel'); res['pred'].plot(label='SARIMAX');\n",
    "    plt.title(f'Forecast SARIMAX PID {pid} (MAE={res[\"sarimax_mae\"]:.1f})'); plt.legend(); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "ids, maes_s, maes_rf = zip(*[(pid,res['sarimax_mae'],res['rf_mae']) for pid,res in results.items()])\n",
    "plt.plot(ids, maes_s, label='SARIMAX MAE'); plt.plot(ids, maes_rf, label='RF MAE'); plt.title('MAE comparatif'); plt.legend(); plt.show()\n",
    "\n",
    "# ROC pour classification retour\n",
    "df_class = df.merge(ret[['quantite_retournee']], on='id_commande', how='left').fillna(0)\n",
    "df_class['retour_flag']=(df_class['quantite_retournee']>0).astype(int)\n",
    "Xc,yc = df_class[features],df_class['retour_flag']\n",
    "Xct, Xcv, yct, ycv = train_test_split(Xc,yc,test_size=0.2,random_state=0,stratify=yc)\n",
    "clf = RandomForestClassifier(n_estimators=100,random_state=0)\n",
    "clf.fit(Xct,yct)\n",
    "probs = clf.predict_proba(Xcv)[:,1]\n",
    "fpr,tpr,_ = roc_curve(ycv,probs)\n",
    "plt.figure(); plt.plot(fpr,tpr); plt.title(f'ROC AUC={roc_auc_score(ycv,probs):.2f}'); plt.xlabel('FPR'); plt.ylabel('TPR'); plt.show()\n",
    "\n",
    "# Export des résultats\n",
    "glob_kpis.to_csv('kpi_supply_chain.csv',index=False)\n",
    "prod_feats.to_csv('clusters_produits.csv',index=False)\n",
    "fourn_feats.to_csv('clusters_fournisseurs.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e46eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Notebook pour optimisation de la chaîne d'approvisionnement\n",
    "\n",
    "# 0. Imports et configuration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, roc_auc_score, roc_curve, precision_score, recall_score, f1_score\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from prophet import Prophet\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "# 1. Chargement et préparation des données\n",
    "# Lecture depuis les CSV extraits\n",
    "df_cmd = pd.read_csv('/base/commandesV2.csv', sep=\";\",parse_dates=['date_commande'])\n",
    "df_exp = pd.read_csv('/mnt/data/expeditions.csv', parse_dates=['date_expedition','date_livraison'])\n",
    "df_ret = pd.read_csv('/mnt/data/retours.csv', parse_dates=['date_retour'])\n",
    "df_stock = pd.read_csv('/mnt/data/stocks.csv', parse_dates=['date_stock'])\n",
    "df_achats = pd.read_csv('/mnt/data/achats.csv', parse_dates=['date_achat'])\n",
    "df_fourn = pd.read_csv('/mnt/data/fournisseurs.csv')\n",
    "df_prod = pd.read_csv('/mnt/data/produits.csv')\n",
    "\n",
    "# Harmonisation texte\n",
    "for df in [df_cmd, df_exp, df_ret, df_stock, df_achats, df_fourn, df_prod]:\n",
    "    for col in df.select_dtypes(include=['object']):\n",
    "        df[col] = df[col].str.strip().str.lower()\n",
    "\n",
    "# 2. Calcul des KPI\n",
    "# 2.1 OTIF\n",
    "df_exp['on_time'] = (df_exp['date_livraison'] <= df_exp['date_attendue']).astype(int)\n",
    "df_exp['full']    = (df_exp['quant_livree'] >= df_exp['quant_attendue']).astype(int)\n",
    "df_exp['otif_flag'] = df_exp['on_time'] * df_exp['full']\n",
    "otif_pct = df_exp['otif_flag'].mean() * 100\n",
    "\n",
    "# 2.2 Taux de retour\n",
    "ret_tot   = df_ret.groupby('id_commande')['quantite_retournee'].sum()\n",
    "deliv_tot = df_exp.groupby('id_commande')['quant_livree'].sum()\n",
    "ret = ret_tot.to_frame('quantite_retournee').join(deliv_tot.to_frame('quant_livree'), how='inner')\n",
    "retour_pct = (ret['quantite_retournee'].sum() / ret['quant_livree'].sum()) * 100\n",
    "\n",
    "# 2.3 Délai moyen expédition\n",
    "df_exp['lead_time'] = (df_exp['date_livraison'] - df_exp['date_expedition']).dt.days\n",
    "delai_moyen = df_exp['lead_time'].mean()\n",
    "\n",
    "# 2.4 Rotation des stocks\n",
    "# consommation annuelle (quantité expédiée) et stock moyen\n",
    "year = df_stock['date_stock'].dt.year.max()\n",
    "consommation = df_exp[df_exp['date_expedition'].dt.year==year]['quant_livree'].sum()\n",
    "stock_deb = df_stock[df_stock['date_stock']==pd.to_datetime(f\"{year}-01-01\")]['niveau_stock'].mean()\n",
    "stock_fin = df_stock[df_stock['date_stock']==pd.to_datetime(f\"{year}-12-31\")]['niveau_stock'].mean()\n",
    "rotation_stock = consommation / ((stock_deb + stock_fin)/2)\n",
    "\n",
    "# 2.5 Coût logistique unitaire\n",
    "cout_unitaire = df_exp['cout_transport'].sum() / df_exp['quant_livree'].sum()\n",
    "\n",
    "# 2.6 Fill Rate\n",
    "fill_rate = df_exp['quant_livree'].sum() / df_exp['quant_attendue'].sum() * 100\n",
    "\n",
    "# Regroup KPI\n",
    "glob_kpis = pd.Series({\n",
    "    'OTIF_%': otif_pct,\n",
    "    'Taux_retour_%': retour_pct,\n",
    "    'Delai_moyen_jours': delai_moyen,\n",
    "    'Rotation_stock': rotation_stock,\n",
    "    'Cout_unitaire': cout_unitaire,\n",
    "    'Fill_rate_%': fill_rate\n",
    "})\n",
    "\n",
    "# 3. EDA rapide\n",
    "glob_kpis.plot(kind='bar', title='KPI Supply Chain'); plt.tight_layout(); plt.show()\n",
    "\n",
    "# 4. Feature Engineering pour ML\n",
    "# Sur base commandes\n",
    "df_ml = df_cmd.merge(ret[['quantite_retournee']], on='id_commande', how='left').fillna(0)\n",
    "# Variables temporelles\n",
    "df_ml['mois'] = df_ml['date_commande'].dt.month\n",
    "df_ml['jour_semaine'] = df_ml['date_commande'].dt.dayofweek\n",
    "# Rolling commandes mensuelles :\n",
    "monthly = df_ml.set_index('date_commande')['quantite_retournee'].resample('M').sum()\n",
    "df_ml = df_ml.merge(monthly.shift(1).rename('roll_1m'),\n",
    "                    left_on=df_ml['date_commande'].dt.to_period('M').dt.to_timestamp(),\n",
    "                    right_index=True, how='left')\n",
    "\n",
    "# 5. Forecasting de la demande\n",
    "# Préparation séries temporelles journalière\n",
    "df_ts = df_cmd.set_index('date_commande').groupby('id_produit')['quantite'].resample('D').sum().fillna(0)\n",
    "results = {}\n",
    "prods = df_ts.index.get_level_values(0).unique()[:3]\n",
    "for pid in prods:\n",
    "    ts = df_ts.loc[pid]\n",
    "    train, test = ts[:-30], ts[-30:]\n",
    "    # SARIMAX\n",
    "    model = SARIMAX(train, order=(1,1,1), seasonal_order=(1,1,1,7)).fit(disp=False)\n",
    "    sar_pred = model.predict(start=test.index[0], end=test.index[-1])\n",
    "    mae_s = mean_absolute_error(test, sar_pred)\n",
    "    # Prophet\n",
    "    df_prop = train.reset_index().rename(columns={'date_commande':'ds','quantite':'y'})\n",
    "    m = Prophet(); m.fit(df_prop)\n",
    "    future = m.make_future_dataframe(periods=30, freq='D')\n",
    "    pr = m.predict(future); pr_pred = pr.set_index('ds')['yhat'].loc[test.index]\n",
    "    mae_p = mean_absolute_error(test, pr_pred)\n",
    "    # RF\n",
    "    lags = pd.concat([train.shift(lag) for lag in [1,7,14]], axis=1)\n",
    "    lags.columns = ['lag1','lag7','lag14']\n",
    "    lags = lags.dropna()\n",
    "    X = lags.values; y = train.loc[lags.index].values\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=0).fit(X,y)\n",
    "    X_test = np.vstack([test.shift(lag).loc[lags.index].values for lag in [1,7,14]]).T\n",
    "    mae_rf = mean_absolute_error(test.loc[lags.index], rf.predict(X_test))\n",
    "    results[pid] = {'SARIMAX_MAE':mae_s,'Prophet_MAE':mae_p,'RF_MAE':mae_rf}\n",
    "\n",
    "# Affichage comparatif\n",
    "res_df = pd.DataFrame(results).T\n",
    "res_df.plot(kind='bar', title='MAE comparatif Forecast'); plt.tight_layout(); plt.show()\n",
    "\n",
    "# 6. Clustering de produits & fournisseurs\n",
    "# Produits\n",
    "df_p = df_achats.groupby('id_produit').agg({'quant_achetee':'sum','prix_unitaire':'mean'}).reset_index()\n",
    "k_p = KMeans(n_clusters=4, random_state=0).fit(df_p[['quant_achetee','prix_unitaire']])\n",
    "df_p['cluster'] = k_p.labels_\n",
    "# Fournisseurs\n",
    "df_f = df_achats.merge(df_fourn[['id_fournisseur','delai_moyen_jours']],on='id_fournisseur')\n",
    "df_f = df_f.groupby('id_fournisseur').agg({'quant_achetee':'sum','delai_moyen_jours':'mean'}).reset_index()\n",
    "k_f = DBSCAN(eps=10, min_samples=2).fit(df_f[['quant_achetee','delai_moyen_jours']])\n",
    "df_f['db_cluster'] = k_f.labels_\n",
    "\n",
    "# PCA pour visualiser clusters fabricants\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "comp = pca.fit_transform(df_f[['quant_achetee','delai_moyen_jours']])\n",
    "df_f['pca1'], df_f['pca2'] = comp[:,0], comp[:,1]\n",
    "plt.figure()\n",
    "plt.scatter(df_f['pca1'], df_f['pca2'], c=df_f['db_cluster'], cmap='tab10')\n",
    "plt.title('Clusters Fournisseurs (DBSCAN)')\n",
    "plt.show()\n",
    "\n",
    "# 7. Modèles de classification des retards\n",
    "df_delay = df_exp.copy()\n",
    "df_delay['late'] = (df_delay['lead_time']>3).astype(int)\n",
    "X = pd.get_dummies(df_delay[['quant_livree','cout_transport','entrepôt']], drop_first=True)\n",
    "y = df_delay['late']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y,test_size=0.3,random_state=42)\n",
    "def eval_clf(m):\n",
    "    m.fit(X_train,y_train)\n",
    "    pred = m.predict(X_test); prob = m.predict_proba(X_test)[:,1]\n",
    "    print(m.__class__.__name__)\n",
    "    print(f\"Precision: {precision_score(y_test,pred):.2f}, Recall: {recall_score(y_test,pred):.2f}, F1: {f1_score(y_test,pred):.2f}, AUC: {roc_auc_score(y_test,prob):.2f}\")\n",
    "for model in [RandomForestClassifier(n_estimators=100), XGBClassifier(use_label_encoder=False, eval_metric='logloss')]:\n",
    "    eval_clf(model)\n",
    "\n",
    "# Courbe ROC pour RF\n",
    "rf_clf = RandomForestClassifier(n_estimators=100).fit(X_train,y_train)\n",
    "fpr,tpr,_ = roc_curve(y_test, rf_clf.predict_proba(X_test)[:,1])\n",
    "plt.figure(); plt.plot(fpr,tpr); plt.title('ROC RF'); plt.show()\n",
    "\n",
    "# 8. Hyperparamétrage example pour RF rég.\n",
    "param_dist = {'n_estimators':[50,100,200],'max_depth':[None,5,10]}\n",
    "rs = RandomizedSearchCV(RandomForestRegressor(), param_dist, n_iter=5, cv=3, scoring='neg_mean_absolute_error',random_state=42)\n",
    "rs.fit(df_p[['quant_achetee']], df_p['quant_achetee'])\n",
    "print('Best Params Reg:', rs.best_params_)\n",
    "```\n",
    "'''\n",
    "_Notes_ :\n",
    "- Remplacement des connexions SQL par lecture CSV.\n",
    "- Fusion des tables selon votre schéma de projet.\n",
    "- Intégration forecasting (SARIMAX, Prophet, RF), clustering (KMeans, DBSCAN), classification retards.\n",
    "- Visualisations matplotlib intégrées.\n",
    "- Exemples d’hyperparamétrage avec RandomizedSearchCV.\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
